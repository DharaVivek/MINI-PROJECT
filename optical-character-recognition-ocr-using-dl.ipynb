{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "**This project is part of Deep Learning/Computer Vision projects in Benha Faculty of Computers and AI (BFCAI)**\n",
    "\n",
    "*Optical Character Recognition (OCR) is one of the critical tasks of computer vision and Artificial Intelligence, due to variety of languages written, variety of fonts for the same languages. our task is to make OCR and text detection for `English` texts*\n",
    "\n",
    "*- our used dataset is `OCR-Dataset` which contains more than `200000` images for alphanumeric characters in both upper and lowercase*\n",
    "*- our target is to detect **LOWERCASE** characters only in the text which are used in the project*\n",
    "\n",
    "> Dataset was generated using 3475 font styles available in Google Fonts. Each alphanumeric character (uppercase, lowercase and numerals) was generated in each font style and stored in a directory. Total dataset size: 2.1 lakh images for 62 classes [OCR-Dataset](https://www.kaggle.com/datasets/harieh/ocr-dataset/)\n",
    "\n",
    "![](https://www.cheggindia.com/wp-content/uploads/2023/08/ocr-full-form.png)\n",
    "\n",
    "## **Table of Content**\n",
    "### *1. Importing Libraries*\n",
    "### *2. Helper Functions & Hyperparameters*\n",
    "### *3. Reading & preparing the dataset*\n",
    "### *4. Modeling*\n",
    "### *5. Evaluation & Comparison*\n",
    "### *6. Post-processing*\n",
    "### *7. Conclusion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:04.216000Z",
     "iopub.status.busy": "2024-06-11T16:45:04.215180Z",
     "iopub.status.idle": "2024-06-11T16:45:04.224923Z",
     "shell.execute_reply": "2024-06-11T16:45:04.223697Z",
     "shell.execute_reply.started": "2024-06-11T16:45:04.215951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default libraries, packages for data management, visualization and Computer vision libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Sklearn package -> function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tensorflow packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, MaxPool2D, \n",
    "                                     BatchNormalization, Flatten, GlobalAveragePooling2D, Input)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.applications import EfficientNetB7, MobileNetV2, VGG19, DenseNet121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Helper Functions & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This section to define all Helper functions through the notebook and any hyperparameters used later for training the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:04.231895Z",
     "iopub.status.busy": "2024-06-11T16:45:04.231506Z",
     "iopub.status.idle": "2024-06-11T16:45:04.248578Z",
     "shell.execute_reply": "2024-06-11T16:45:04.247288Z",
     "shell.execute_reply.started": "2024-06-11T16:45:04.231865Z"
    }
   },
   "outputs": [],
   "source": [
    "def directory_to_df(path : str):\n",
    "    \"\"\"\n",
    "    This function to retrieve all images from targeted folder in a file, the\n",
    "    folder must be divided hirarchally in which each class contains its images individually.\n",
    "    ________________________________________________________________________________________________\n",
    "    Arguments-\n",
    "    \n",
    "    path: String -> the main folder directory that contains train/test folders\n",
    "    \n",
    "    ________________________________________________________________________________________________\n",
    "    Return-\n",
    "    \n",
    "    DataFrame: contains the images path and label corresponding to every image\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    chars = 'abcdABCD'    # to include lowercase letters only\n",
    "    for cls in os.listdir(path):\n",
    "        cls_path = os.path.join(path,cls)\n",
    "        cls_name = cls.split('_')[0]\n",
    "        if not cls_name in chars:\n",
    "            continue\n",
    "        for img_path in os.listdir(cls_path):\n",
    "            direct = os.path.join(cls_path,img_path)\n",
    "            df.append([direct,cls_name])\n",
    "    \n",
    "    df = pd.DataFrame(df, columns=['image','label'])\n",
    "    print(\"The number of samples found:\",len(df))\n",
    "    return df.copy()\n",
    "\n",
    "def read_image(path):\n",
    "    \"\"\"\n",
    "    Read an image from specified directory\n",
    "    _____________________________________________________________\n",
    "    Arguments:\n",
    "    \n",
    "    path: String -> a directory of the image\n",
    "    _____________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    image: numpy.array of the image\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def show_image(img, label=None) -> None:\n",
    "    \"\"\"\n",
    "    This function to display any image\n",
    "    _________________________________________________________\n",
    "    Arguements:\n",
    "    \n",
    "    img: numpy.array of N-D\n",
    "    \n",
    "    label: String -> the title/label added with the image, Default= None\n",
    "    _________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    plt.imshow()\n",
    "    \"\"\"\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "def clbck(model_name):\n",
    "    # The function is defined to make the callbacks for training the models\n",
    "    ERLY = EarlyStopping(patience=10, min_delta=0.01, start_from_epoch=10, verbose=1)\n",
    "    RD = ReduceLROnPlateau(patience=5, min_delta=0.01, factor=0.5)\n",
    "    CHK = ModelCheckpoint(f'{model_name}_model.h5',verbose=1, save_best_only=True)\n",
    "    return [ERLY,RD,CHK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:04.250826Z",
     "iopub.status.busy": "2024-06-11T16:45:04.250458Z",
     "iopub.status.idle": "2024-06-11T16:45:04.357900Z",
     "shell.execute_reply": "2024-06-11T16:45:04.356397Z",
     "shell.execute_reply.started": "2024-06-11T16:45:04.250794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined hyperparameters\n",
    "IMG_SHAPE = (32,32)\n",
    "IMG_SIZE = (32,32,3)\n",
    "BATCH_SIZE = 32\n",
    "opt = Adam(learning_rate=0.00001, epsilon=1e-6)\n",
    "loss = 'categorical_crossentropy'\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Reading & preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:04.360446Z",
     "iopub.status.busy": "2024-06-11T16:45:04.360044Z",
     "iopub.status.idle": "2024-06-11T16:45:05.571498Z",
     "shell.execute_reply": "2024-06-11T16:45:05.570309Z",
     "shell.execute_reply.started": "2024-06-11T16:45:04.360411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples found: 26998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/ocr-dataset/dataset/c_L/C_L_1241...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/ocr-dataset/dataset/c_L/C_L_3096...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/ocr-dataset/dataset/c_L/C_L_3184...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/ocr-dataset/dataset/c_L/C_L_2981...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/ocr-dataset/dataset/c_L/C_L_1635...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image label\n",
       "0  /kaggle/input/ocr-dataset/dataset/c_L/C_L_1241...     c\n",
       "1  /kaggle/input/ocr-dataset/dataset/c_L/C_L_3096...     c\n",
       "2  /kaggle/input/ocr-dataset/dataset/c_L/C_L_3184...     c\n",
       "3  /kaggle/input/ocr-dataset/dataset/c_L/C_L_2981...     c\n",
       "4  /kaggle/input/ocr-dataset/dataset/c_L/C_L_1635...     c"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset in dataframe \n",
    "main_path = '/kaggle/input/ocr-dataset/dataset'\n",
    "df = directory_to_df(main_path)                   # convert the dataset into df of two columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:05.573470Z",
     "iopub.status.busy": "2024-06-11T16:45:05.573014Z",
     "iopub.status.idle": "2024-06-11T16:45:05.596106Z",
     "shell.execute_reply": "2024-06-11T16:45:05.594804Z",
     "shell.execute_reply.started": "2024-06-11T16:45:05.573424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "C    3473\n",
       "D    3473\n",
       "A    3473\n",
       "B    3473\n",
       "a    3281\n",
       "d    3277\n",
       "c    3274\n",
       "b    3274\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Splitting the dataframe\n",
    "- The dataframe is splitted to get 70% of the dataset for `training` , and 30% for `testing`\n",
    "- The training set is splitted into `training` and `validation` to enhance the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:05.599686Z",
     "iopub.status.busy": "2024-06-11T16:45:05.599310Z",
     "iopub.status.idle": "2024-06-11T16:45:05.620622Z",
     "shell.execute_reply": "2024-06-11T16:45:05.619364Z",
     "shell.execute_reply.started": "2024-06-11T16:45:05.599648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting for training & testing (70,30 respectively)\n",
    "X, y = df['image'], df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.30, random_state=41)\n",
    "training_df = pd.concat((X_train,y_train), axis=1)\n",
    "testing_df = pd.concat((X_test,y_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:05.622448Z",
     "iopub.status.busy": "2024-06-11T16:45:05.622049Z",
     "iopub.status.idle": "2024-06-11T16:45:05.639456Z",
     "shell.execute_reply": "2024-06-11T16:45:05.637983Z",
     "shell.execute_reply.started": "2024-06-11T16:45:05.622405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting for training & validation (75,25 respectively) -> the training set size = 52.5%\n",
    "X, y = training_df['image'], training_df['label']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y , test_size=0.25, random_state=41)\n",
    "training_df = pd.concat((X_train,y_train), axis=1)\n",
    "validation_df = pd.concat((X_valid,y_valid), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Creating generators\n",
    "- The `ImageDataGenerators` is used for data augmentation, the augmentation is required since the `OCR` can work with different brightness which is not included in the dataset.\n",
    "- Also, it enhance the RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:05.641469Z",
     "iopub.status.busy": "2024-06-11T16:45:05.641092Z",
     "iopub.status.idle": "2024-06-11T16:45:32.224528Z",
     "shell.execute_reply": "2024-06-11T16:45:32.223275Z",
     "shell.execute_reply.started": "2024-06-11T16:45:05.641435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14173 validated image filenames belonging to 8 classes.\n",
      "Found 4725 validated image filenames belonging to 8 classes.\n",
      "Found 8100 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating generators\n",
    "gen = ImageDataGenerator(dtype=np.int32, brightness_range=[0.0,1.0], fill_mode='nearest')\n",
    "gen2 = ImageDataGenerator(dtype=np.int32, fill_mode='nearest')\n",
    "train_gen = gen.flow_from_dataframe(training_df, x_col='image',y_col='label', batch_size=BATCH_SIZE, \n",
    "                                   target_size=IMG_SHAPE)\n",
    "valid_gen = gen2.flow_from_dataframe(validation_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                        target_size=IMG_SHAPE, shuffle=False)\n",
    "test_gen = gen2.flow_from_dataframe(testing_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                       target_size=IMG_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:32.226616Z",
     "iopub.status.busy": "2024-06-11T16:45:32.226180Z",
     "iopub.status.idle": "2024-06-11T16:45:32.233483Z",
     "shell.execute_reply": "2024-06-11T16:45:32.232127Z",
     "shell.execute_reply.started": "2024-06-11T16:45:32.226574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making a mapping of the classes and the inverse for later processings\n",
    "mapping = train_gen.class_indices\n",
    "mapping_inverse = dict(map(lambda x: tuple(reversed(x)), mapping.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:32.235387Z",
     "iopub.status.busy": "2024-06-11T16:45:32.234905Z",
     "iopub.status.idle": "2024-06-11T16:45:32.627606Z",
     "shell.execute_reply": "2024-06-11T16:45:32.626266Z",
     "shell.execute_reply.started": "2024-06-11T16:45:32.235343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the image: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reading a sample from the dataset\n",
    "BATCH_NUM = 10\n",
    "IMG_NUM = 2      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:32.629968Z",
     "iopub.status.busy": "2024-06-11T16:45:32.629474Z",
     "iopub.status.idle": "2024-06-11T16:45:32.990166Z",
     "shell.execute_reply": "2024-06-11T16:45:32.988809Z",
     "shell.execute_reply.started": "2024-06-11T16:45:32.629915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the image: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reading another sample from the dataset\n",
    "BATCH_NUM = 65\n",
    "IMG_NUM = 30      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:32.994394Z",
     "iopub.status.busy": "2024-06-11T16:45:32.993990Z",
     "iopub.status.idle": "2024-06-11T16:45:33.285797Z",
     "shell.execute_reply": "2024-06-11T16:45:33.284679Z",
     "shell.execute_reply.started": "2024-06-11T16:45:32.994359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom CNN\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(Input(shape=IMG_SIZE, batch_size=BATCH_SIZE, name='Input'))\n",
    "CNN_model.add(Conv2D(3, (3,3), strides=1, activation='relu', padding='same'))\n",
    "CNN_model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "CNN_model.add(MaxPool2D((3,3)))\n",
    "CNN_model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "CNN_model.add(Dropout(0.2))\n",
    "CNN_model.add(Conv2D(256, (3,3), strides=2, activation='relu', padding='same'))\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "CNN_model.add(Dropout(0.2))\n",
    "CNN_model.add(Conv2D(1024, (2,2), activation='relu', padding='same'))\n",
    "CNN_model.add(MaxPool2D(2,2))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(1024, activation='selu'))\n",
    "CNN_model.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:33.287446Z",
     "iopub.status.busy": "2024-06-11T16:45:33.287113Z",
     "iopub.status.idle": "2024-06-11T16:45:33.338258Z",
     "shell.execute_reply": "2024-06-11T16:45:33.337069Z",
     "shell.execute_reply.started": "2024-06-11T16:45:33.287417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (32, 32, 32, 3)           84        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (32, 30, 30, 128)         3584      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (32, 10, 10, 128)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (32, 8, 8, 256)           295168    \n",
      "                                                                 \n",
      " dropout (Dropout)           (32, 8, 8, 256)           0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (32, 4, 4, 256)           590080    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (32, 2, 2, 256)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (32, 2, 2, 512)           1180160   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (32, 2, 2, 512)           0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (32, 2, 2, 1024)          2098176   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (32, 1, 1, 1024)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 1024)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (32, 1024)                1049600   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 8)                   8200      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5225052 (19.93 MB)\n",
      "Trainable params: 5225052 (19.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:33.340177Z",
     "iopub.status.busy": "2024-06-11T16:45:33.339745Z",
     "iopub.status.idle": "2024-06-11T16:45:33.362710Z",
     "shell.execute_reply": "2024-06-11T16:45:33.361491Z",
     "shell.execute_reply.started": "2024-06-11T16:45:33.340140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default parameters of adam will be used for the custom CNN\n",
    "CNN_model.compile(optimizer=Adam(), loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:45:33.364840Z",
     "iopub.status.busy": "2024-06-11T16:45:33.364470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "443/443 [==============================] - ETA: 0s - loss: 1.0349 - accuracy: 0.6933\n",
      "Epoch 1: val_loss improved from inf to 0.49488, saving model to CustomCnn_model.h5\n",
      "443/443 [==============================] - 213s 477ms/step - loss: 1.0349 - accuracy: 0.6933 - val_loss: 0.4949 - val_accuracy: 0.8487 - lr: 0.0010\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/443 [==================>...........] - ETA: 1:07 - loss: 0.4053 - accuracy: 0.8706"
     ]
    }
   ],
   "source": [
    "# different num. of epochs will be given for better convergence for the Custom CNN\n",
    "history = CNN_model.fit(train_gen, epochs=20, validation_data=valid_gen, callbacks=clbck(\"CustomCnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"Custom CNN Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the Custom CNN for the testing set for the evaluation\n",
    "prediction = CNN_model.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe Custom CNN Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Post-Processing\n",
    "#### **THIS SECTION IS ABOUT `Computer Vision` TECHNIQUES**\n",
    "\n",
    "*the image will be taken from the user/externally to be processed and input into our model. some **LOW LEVEL COMPUTER VISION TECHNIQUES** are used in order to enhance the input image, wheter it is text or handwritten, these techniques are the following:*\n",
    "\n",
    "- **Binarization:** The binarization function applies Otsu's binarization to the grayscale image, producing a binary image.\n",
    "\n",
    "- **Dilate:** The dilate function performs morphological dilation on the binary image. The degree of dilation is adjusted based on whether processing `words` or `characters`.\n",
    "\n",
    "- **Find Rectangles:** The find_rect function identifies bounding rectangles in the binary image, sorting them based on their **x-coordinate (From Left-to-Right)**.\n",
    "\n",
    "- **Extract Characters:** The extract function uses the aforementioned techniques to extract characters from the image. It identifies words **FIRST**, then extracts characters from each word, and with the use of the pre-trained deep learning model to recognize each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision - Low level techniques\n",
    "def load_model():\n",
    "    model_path = '/kaggle/working/CustomCnn_model.h5'\n",
    "    model = tf.keras.saving.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def convert_2_gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_image\n",
    "\n",
    "def binarization(image):\n",
    "    img, thresh = cv2.threshold(image, 0,255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "    return img, thresh\n",
    "\n",
    "def dilate(image, words= False):\n",
    "    img = image.copy()\n",
    "    m = 3\n",
    "    n = m - 2                   # n less than m for Vertical structuring element to dilate chars\n",
    "    itrs = 4\n",
    "    if words:\n",
    "        m = 6\n",
    "        n = m\n",
    "        itrs = 3\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (n, m))\n",
    "    dilation = cv2.dilate(img, rect_kernel, iterations = itrs)\n",
    "    return dilation\n",
    "\n",
    "def find_rect(image):\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    rects = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)  # Extract the bounding rectangle coordinates of each countour\n",
    "        rects.append([x,y,w,h])\n",
    "        \n",
    "    sorted_rects = list(sorted(rects, key=lambda x: x[0])) # Sorting the rects from Left-to-Right\n",
    "    return sorted_rects\n",
    "\n",
    "def extract(image):\n",
    "    model = load_model()\n",
    "    chars = []              # a list to store recognized characters\n",
    "    \n",
    "    image_cpy = image.copy()\n",
    "    _, bin_img = binarization(convert_2_gray(image_cpy))\n",
    "    full_dil_img = dilate(bin_img,words=True)\n",
    "    words = find_rect(full_dil_img)                       # Recognized words within the image \n",
    "    del _, bin_img, full_dil_img                          # for better memory usage\n",
    "    \n",
    "    for word in words:\n",
    "        x,y,w,h = word                                    # coordinates of the word\n",
    "        img = image_cpy[y:y+h, x:x+w]\n",
    "        \n",
    "        _, bin_img = binarization(convert_2_gray(img))\n",
    "        dil_img = dilate(bin_img)\n",
    "        char_parts = find_rect(dil_img)                     # Recognized chars withtin the word\n",
    "        cv2.rectangle(image, (x,y),(x+w,y+h), (0,255,0), 3) # draw a green rectangle around the word\n",
    "        \n",
    "        del _, bin_img, dil_img\n",
    "        \n",
    "        for char in char_parts:    \n",
    "            x,y,w,h = char\n",
    "            ch = img[y:y+h, x:x+w]\n",
    "            \n",
    "            empty_img = np.full((32,32,1),255, dtype=np.uint8) # a white image used for resize with filling\n",
    "            x,y = 3,3                                          # starting indecies\n",
    "            resized = cv2.resize(ch, (16,22), interpolation=cv2.INTER_CUBIC)\n",
    "            gray = convert_2_gray(resized)\n",
    "            empty_img[y:y+22, x:x+16,0] = gray.copy()          # integrate the recognized char into the white image\n",
    "            gray = cv2.cvtColor(empty_img, cv2.COLOR_GRAY2RGB)\n",
    "            gray = gray.astype(np.int32)\n",
    "            \n",
    "            predicted = mapping_inverse[np.argmax(model.predict(np.array([gray]), verbose=-1))]\n",
    "            chars.append(predicted)                            # append the character into the list\n",
    "            \n",
    "            del ch, resized, gray, empty_img\n",
    "        chars.append(' ')  # at the end of each iteration (end of word) append a space\n",
    "        \n",
    "    del model\n",
    "    show_image(image)\n",
    "    return ''.join(chars[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 1\n",
    "img = read_image('/kaggle/input/abcd-chars/WhatsApp Image 2024-06-10 at 10.21.14.jpeg')\n",
    "text = extract(img)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 2\n",
    "img2 = read_image('/kaggle/input/abcd-chars/WhatsApp Image 2024-06-10 at 10.21.37.jpeg')\n",
    "text = extract(img2)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 3\n",
    "img3 = read_image('/kaggle/input/abcd-chars/WhatsApp Image 2024-06-10 at 10.22.10.jpeg')\n",
    "text = extract(img3)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 4\n",
    "img4 = read_image('/kaggle/input/abcd-chars/WhatsApp Image 2024-06-10 at 10.22.28.jpeg')\n",
    "text = extract(img4)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1176843,
     "sourceId": 1972672,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1634134,
     "sourceId": 2685160,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3364134,
     "sourceId": 5850209,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5193535,
     "sourceId": 8666727,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5193584,
     "sourceId": 8666790,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
